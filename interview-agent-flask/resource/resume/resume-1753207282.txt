基于 RAG 的大模型智能问答系统需求规格说明书  
一、引言  
1.1 项目背景  
随着大模型技术发展， 领域知识问答 （如学科、产品、客服场景）需结合专业知识库
实现精准回答。传统问答系统无法高效处理多格式文档与自然语言交互，因此需要构
建 “检索增强生成（ RAG）+ 大模型” 的智能问答系统，整合知识管理与智能交互
能力。 
1.2 项目目标  
• 实现多格式文档的知识库自动化构建 ，支持知识更新与管理。  
• 提供自然语言精准检索与智能回答 ，结合对话上下文生成连贯回复。  
• 搭建可视化交互界面 ，降低知识维护与问答使用门槛。  
1.3 范围界定  
• 功能范围 ：知识库构建（文档导入  / 解析） 、知识管理（分类  / 标注） 、智能
问答（意图解析  / 答案生成） 、对话上下文管理。  
• 业务范围 ：聚焦特定领域（如医疗、教育、产品客服） ，暂不支持跨领域通用
问答。 
二、用户需求说明 ——User Story 设计 
1. 普通用户  - 智能问答  
User Story  验收标准  
作为普通用户，我想  用自然语言提问 （如 
“Python 循环怎么写？ ”） ，系统返回答案。  1. 支持输入中文问题 ； 
2. 系统能根据知识库的内
容，返回至少  1 条答案。 
2. 普通用户  - 历史记录查看  
User Story  验收标准  
作为普通用户，我想  查看最近  3 条问答记录 ，知
道之前问了什么。  1. 页面展示  “问题、答案、
时间”； 
2. 至少保留最近  3 条记录；   
3. 知识管理员  - 文档导入  
User Story  验收标准  
作为知识管理员，我想  上传不同格式的文档 （如
学习笔记） ，系统存到知识库。  1. 支持单文件上传
（≤500MB）； 
2. 能提取不同格式文件里的
文字内容（忽略格式） ；  
3. 导入成功提示  “已保
存”，失败提示  “格式错
误”。 
4. 知识管理员  - 知识库概览  
User Story  验收标准  
作为知识管理员，我想  看到已上传的文档列表
（如标题、数量） ，知道知识库 基本内容。 1. 列表展示  “文档标题、
导入时间 ”； 
2. 至少显示  5 条记录；  
3. 能够添加，删除和标记知
识库内容 
三、需求分析建模 
3.1 交互逻辑建模：模块协作流程  
本模型聚焦系统响应用户请求的动态协作过程，以  “角色 / 模块交互时序 ” 为脉
络，清晰界定各组件职责，其核心逻辑契合需求向技术转化的关键路径：  
1、用户作为需求发起与结果接收的核心主体，提交自然语言问题并获取答案，构成  
“提问 - 答问” 基础业务闭环。系统承担调度中枢职能，一方面转发用户问题至语
义理解模块，启动意图与关键信息解析流程；另一方面整合知识库检索结果与大模型
输出，生成最终回答，并通过记录对话上下文，保障多轮交互场景下的语义连贯性，
支持用户追问时关联历史问题。  
2、语义理解模块聚焦自然语言解析，将用户问题转化为  “意图（如知识查询、流程
咨询等）  + 关键信息（实体、关键词） ” 的结构化形式，为知识检索提供精准依
据，是实现用户问题有效理解的核心环节。  
3、知识库承载知识检索功能，接收语义理解模块输出的关键信息，检索并返回匹配的
知识片段（如文档段落、 FAQ 内容） ，作为  RAG 架构中 “检索（Retrieval ）” 环节
的核心载体，为答案生成提供知识支撑。  
4、大模型负责答案生成，结合  “知识片段  + 原始问题 ”，利用其生成能力输出自然
语言回答，是  RAG 架构中 “生成（Generation ）” 环节的核心载体，实现知识与问
题的融合响应。  
 
5、通过上述链式协作，完整呈现  “解析问题 →检索知识 →生成答案 ” 的 RAG 核心
逻辑，同时依托  “对话上下文记录 ”，覆盖 “单轮问答  + 连续对话 ” 业务场景，
为系统功能设计明确交互流程框架。  
3.2 数据流转建模：分层数据加工流程  
本模型从静态数据视角出发，以  “分层数据流向 ” 为线索，清晰呈现系统的数据加
工路径与模块处理逻辑，其结构契合需求转化为数据资产的内在规律：  
1、用户侧构成业务数据闭环，以用户自然语言问题为输入，以系统生成答案为输出，
通过 “输入 - 输出” 双向流动，直观体现用户与系统交互的本质数据流转。  
2、系统处理层拆分为  “知识构建 ” 和 “问答服务 ” 双生命周期核心子流程：在知
识构建阶段（准备期） ，文档导入后经知识解析存储环节，完成  “非结构化文档 →结
构化知识 ” 的转换，最终持久化到知识库，为问答功能储备数据基础；在问答服务阶
段（运行期） ，用户问题依次通过语义理解、知识检索（从知识库提取数据  ） 、大模型
生成环节，实现  “用户问题 →知识驱动回答 ” 的完整加工链路，支撑问答功能的动
态运行。  
3、知识库作为数据中枢，在知识构建阶段接收  “知识解析存储 ” 输出，完成知识持
久化；在问答阶段向  “知识检索 ” 提供查询服务，为问答过程供给知识片段，串联
起知识从构建到应用的全流程，体现数据对业务的支撑价值。各模块聚焦单一数据加
工任务（如  “知识解析存储 ” 专注文档到知识的转换  ） ，遵循高内聚、低耦合设计
原则，为系统数据处理架构提供清晰边界。  
图3.1 时序图

 
图3.2 数据流图  
3.2智能文献问答系统数据  
1.系统ER图 
 
图3.3 系统ER图 
2．数据字典 
1、会话数据结构  
数据项 数据类型  描述 示例值 
conversation  List[str]  对话历史记
录，按 “用户
提问 - 系统回
答” 顺序存储  ["用户: 如何使用
该系统？ ", "系统: 
请先上传文档 ..."] 
uploaded_files  List[Dict]  已上传文件列
表，每个文件[{"id": "f123", 
"name": " 手

包含元信息和
内容 册.pdf", 
"content": " 文档
内容..."}] 
knowledge_base  List[Dict]  知识库片段，
存储文档拆分
后的内容及来
源信息 [{"source": " 手
册.pdf", 
"content": " 第一
章...", "type": 
"pdf"}] 
deleted_files  List[Dict]  已删除文件回
收站，包含删
除时间等元信
息 [{"id": "f123", 
"deleted_time": 
"2025-06-24 
10:00:00"}]  
embedding_mode
l SentenceTransfor
mer 语义嵌入模
型，用于生成
文本向量  SentenceTransform
er('paraphrase -
multilingual -
MiniLM-L12-v2') 
knowledge_vect
ors np.ndarray  知识库片段的
向量表示，用
于相似度检索  array([0.1, 
0.2, ...], 
dtype=float32)  
vector_db  Chroma 向量数据库实
例，存储文本
向量及元数据  Chroma(embedding_
function=..., 
persist_directory
="./chroma_db")  
2、上传文件数据结构  
字段 数据类型  描述 约束条件  
id str 文件唯一标识（ MD5 
哈希前 8 位） 主键，长度  8 字符 
name str 原始文件名  非空，示例： "用户手
册.pdf" 
type str 文件 MIME 类型 非空，示例：
"application/pdf"  
content str 文件解析后的文本内 非空，存储纯文本  
容 
upload_time  str 上传时间（ YYYY-MM-
DD HH:MM:SS 格式） 非空，自动填充当前时间  
tags List[str]  文件标记（如  “重
要”“待审核”） 可选，默认为  ["新上传
"] 
3、知识库片段数据结构  
字段 数据类型  描述 约束条件  
source str 来源文件名  非空，示例： "用户手
册.pdf" 
source_id  str 来源文件  ID（关联
uploaded_files.id ） 非空，外键关联  
content str 文档拆分后的文本片段  非空，长度由
chunk_size 决定 
type str 文档类型（ txt/pdf/docx 
等） 非空，示例： "pdf" 
4、向量数据库元数据结构  
字段 数据类型  描述 存储位置  
source str 来源文件名  Chroma 元数据 
source_id  str 来源文件  ID Chroma 元数据 
type str 文档类型  Chroma 元数据 
upload_time  str 上传时间  Chroma 元数据 
5、对话历史数据结构  
字段 数据类
型 描述 示例值 
role str 对话角色（ "user" 或 
"assistant" ） "user" 
content str 对话内容  "如何上传文档？
" 
6、系统配置数据结构  
字段 数据类型  描述 默认值 
text_splitter  RecursiveChar
acterTextSpli
tter 文本拆分器实
例，用于文档
分段 chunk_size=1000, 
chunk_overlap=200  
embedding_model
_langchain  HuggingFaceEm
beddings  LangChain 嵌
入模型实例，
用于向量生成  model_name="Ganymede
Nil/text2vec -large-
chinese"  
一、系统开发环境  
1. 核心工具与框架  
组件 用途 版本/配置 
Python 主开发语言  3.8+（需支持 typing.Union 等现代语法）  
Streamlit  构建交互式
Web界面 1.32.0+（需支持会话状态管理）  
LangChain  文本分割、
向量检索与
问答链构建  集成RecursiveCharacterTextSplitter 、
Chroma等组件 
Sentence -
Transformers  生成文本语
义向量 paraphrase -multilingual -MiniLM-L12-v2
（多语言模型，约 1.2GB） 
DeepSeek API  答案生成引
擎 兼容OpenAI格式（openai==1.12.0 ） 
二、系统运行环境  
1. 硬件要求  
场景 最低配置  推荐配置  
开发环
境 CPU 4核 / 内存 8GB CPU 8核 / 内存 16GB 
生产环
境 CPU 8核 / 内存 16GB / 存储 
100GB GPU加速（如 NVIDIA T4 16GB
显存） 
2. 软件依赖  
类别 要求 
操作系
统 Windows 10+/Linux/macOS （需测试各平台兼容性）  
容器化 Docker 20.10+ （可选，用于部署）  
网络 带宽≥10Mbps ，开放端口： 8501（Streamlit ）、8000（Chroma 
DB） 
 
三、系统依赖项清单  
1. Python核心依赖  
streamlit==1.32.0          Web界面框架  
langchain==0.1.0           文本处理与检索  
sentence -transformers==2.3.0   语义向量模型  
openai==1.12.0             DeepSeek API 调用 
PyPDF2==3.0.0              PDF解析 
python-docx==0.8.11        DOCX解析 
python-pptx==0.6.23        PPTX解析 
nltk==3.8.1                文本预处理（需下载停用词库）  
chromadb==0.4.0            向量数据库  
2. 外部服务依赖  
DeepSeek API ：需配置有效的 API密钥（openai.api_key ）和端点
（openai.base_url ） 
Chroma DB ：本地持久化存储（默认目录 ./chroma_db ） ，无需额外服务  
3. 首次运行初始化  
下载NLTK资源（需联网）  
预下载语义模型（约 1.2GB） 
 
